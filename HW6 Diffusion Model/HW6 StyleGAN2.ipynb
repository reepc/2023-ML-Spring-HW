{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n# HW6 Diffusion Model\n\n**Sources:**\n- Github implementation [Denoising Diffusion Pytorch](https://github.com/lucidrains/denoising-diffusion-pytorch)\n- Papers on Diffusion models ([Dhariwal, Nichol, 2021], [Ho et al., 2020] ect.)\n","metadata":{"id":"HhIgGq3za0yh"}},{"cell_type":"markdown","source":"## Import Packages and Set Seeds","metadata":{"id":"wLHSIArLcFK0"}},{"cell_type":"code","source":"!pip install einops\n!pip install transformers\n!pip install ema_pytorch\n!pip install accelerate","metadata":{"id":"s1xegyILIuLz","outputId":"b71ce929-c0bd-4ff0-ff53-cb01b421c2e9","execution":{"iopub.status.busy":"2023-03-30T06:17:55.373297Z","iopub.execute_input":"2023-03-30T06:17:55.374214Z","iopub.status.idle":"2023-03-30T06:18:43.218774Z","shell.execute_reply.started":"2023-03-30T06:17:55.374161Z","shell.execute_reply":"2023-03-30T06:18:43.217348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install stylegan2_pytorch\n# !stylegan2_pytorch --data /kaggle/input/diffusion/faces/faces --name HW-6-USE-Style2GAN --image-size 64 --num-train-steps 10000 --load-from ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Set","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport math\nimport fire\nimport json\n\nfrom tqdm import tqdm\nfrom math import floor, log2\nfrom random import random\nfrom shutil import rmtree\nfrom functools import partial\nimport multiprocessing\nfrom contextlib import contextmanager, ExitStack\n\nimport numpy as np\n\nimport torch\nfrom torch import nn, einsum\nfrom torch.utils import data\nfrom torch.optim import Adam\nimport torch.nn.functional as F\nfrom torch.autograd import grad as torch_grad\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\nfrom einops import rearrange, repeat\nfrom kornia.filters import filter2d\n\nimport torchvision\nfrom torchvision import transforms\nfrom stylegan2_pytorch.version import __version__\nfrom stylegan2_pytorch.diff_augment import DiffAugment\n\nfrom vector_quantize_pytorch import VectorQuantize\n\nfrom PIL import Image\nfrom pathlib import Path\n\ntry:\n    from apex import amp\n    APEX_AVAILABLE = True\nexcept:\n    APEX_AVAILABLE = False\n\nimport aim\n\nassert torch.cuda.is_available(), 'You need to have an Nvidia GPU with CUDA installed.'\n\n\n# constants\n\nNUM_CORES = multiprocessing.cpu_count()\nEXTS = ['jpg', 'jpeg', 'png']\n\n# helper classes\n\nclass NanException(Exception):\n    pass\n\nclass EMA():\n    def __init__(self, beta):\n        super().__init__()\n        self.beta = beta\n    def update_average(self, old, new):\n        if not exists(old):\n            return new\n        return old * self.beta + (1 - self.beta) * new\n\nclass Flatten(nn.Module):\n    def forward(self, x):\n        return x.reshape(x.shape[0], -1)\n\nclass RandomApply(nn.Module):\n    def __init__(self, prob, fn, fn_else = lambda x: x):\n        super().__init__()\n        self.fn = fn\n        self.fn_else = fn_else\n        self.prob = prob\n    def forward(self, x):\n        fn = self.fn if random() < self.prob else self.fn_else\n        return fn(x)\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n    def forward(self, x):\n        return self.fn(x) + x\n\nclass ChanNorm(nn.Module):\n    def __init__(self, dim, eps = 1e-5):\n        super().__init__()\n        self.eps = eps\n        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))\n        self.b = nn.Parameter(torch.zeros(1, dim, 1, 1))\n\n    def forward(self, x):\n        var = torch.var(x, dim = 1, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = 1, keepdim = True)\n        return (x - mean) / (var + self.eps).sqrt() * self.g + self.b\n\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.fn = fn\n        self.norm = ChanNorm(dim)\n\n    def forward(self, x):\n        return self.fn(self.norm(x))\n\nclass PermuteToFrom(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n    def forward(self, x):\n        x = x.permute(0, 2, 3, 1)\n        out, *_, loss = self.fn(x)\n        out = out.permute(0, 3, 1, 2)\n        return out, loss\n\nclass Blur(nn.Module):\n    def __init__(self):\n        super().__init__()\n        f = torch.Tensor([1, 2, 1])\n        self.register_buffer('f', f)\n    def forward(self, x):\n        f = self.f\n        f = f[None, None, :] * f [None, :, None]\n        return filter2d(x, f, normalized=True)\n\n# attention\n\nclass DepthWiseConv2d(nn.Module):\n    def __init__(self, dim_in, dim_out, kernel_size, padding = 0, stride = 1, bias = True):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(dim_in, dim_in, kernel_size = kernel_size, padding = padding, groups = dim_in, stride = stride, bias = bias),\n            nn.Conv2d(dim_in, dim_out, kernel_size = 1, bias = bias)\n        )\n    def forward(self, x):\n        return self.net(x)\n\nclass LinearAttention(nn.Module):\n    def __init__(self, dim, dim_head = 64, heads = 8):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.nonlin = nn.GELU()\n        self.to_q = nn.Conv2d(dim, inner_dim, 1, bias = False)\n        self.to_kv = DepthWiseConv2d(dim, inner_dim * 2, 3, padding = 1, bias = False)\n        self.to_out = nn.Conv2d(inner_dim, dim, 1)\n\n    def forward(self, fmap):\n        h, x, y = self.heads, *fmap.shape[-2:]\n        q, k, v = (self.to_q(fmap), *self.to_kv(fmap).chunk(2, dim = 1))\n        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> (b h) (x y) c', h = h), (q, k, v))\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n\n        out = self.nonlin(out)\n        return self.to_out(out)\n\n# one layer of self-attention and feedforward, for images\n\nattn_and_ff = lambda chan: nn.Sequential(*[\n    Residual(PreNorm(chan, LinearAttention(chan))),\n    Residual(PreNorm(chan, nn.Sequential(nn.Conv2d(chan, chan * 2, 1), leaky_relu(), nn.Conv2d(chan * 2, chan, 1))))\n])\n\n# helpers\n\ndef exists(val):\n    return val is not None\n\n@contextmanager\ndef null_context():\n    yield\n\ndef combine_contexts(contexts):\n    @contextmanager\n    def multi_contexts():\n        with ExitStack() as stack:\n            yield [stack.enter_context(ctx()) for ctx in contexts]\n    return multi_contexts\n\ndef default(value, d):\n    return value if exists(value) else d\n\ndef cycle(iterable):\n    while True:\n        for i in iterable:\n            yield i\n\ndef cast_list(el):\n    return el if isinstance(el, list) else [el]\n\ndef is_empty(t):\n    if isinstance(t, torch.Tensor):\n        return t.nelement() == 0\n    return not exists(t)\n\ndef raise_if_nan(t):\n    if torch.isnan(t):\n        raise NanException\n\ndef gradient_accumulate_contexts(gradient_accumulate_every, is_ddp, ddps):\n    if is_ddp:\n        num_no_syncs = gradient_accumulate_every - 1\n        head = [combine_contexts(map(lambda ddp: ddp.no_sync, ddps))] * num_no_syncs\n        tail = [null_context]\n        contexts =  head + tail\n    else:\n        contexts = [null_context] * gradient_accumulate_every\n\n    for context in contexts:\n        with context():\n            yield\n\ndef loss_backwards(fp16, loss, optimizer, loss_id, **kwargs):\n    if fp16:\n        with amp.scale_loss(loss, optimizer, loss_id) as scaled_loss:\n            scaled_loss.backward(**kwargs)\n    else:\n        loss.backward(**kwargs)\n\ndef gradient_penalty(images, output, weight = 10):\n    batch_size = images.shape[0]\n    gradients = torch_grad(outputs=output, inputs=images,\n                           grad_outputs=torch.ones(output.size(), device=images.device),\n                           create_graph=True, retain_graph=True, only_inputs=True)[0]\n\n    gradients = gradients.reshape(batch_size, -1)\n    return weight * ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n\ndef calc_pl_lengths(styles, images):\n    device = images.device\n    num_pixels = images.shape[2] * images.shape[3]\n    pl_noise = torch.randn(images.shape, device=device) / math.sqrt(num_pixels)\n    outputs = (images * pl_noise).sum()\n\n    pl_grads = torch_grad(outputs=outputs, inputs=styles,\n                          grad_outputs=torch.ones(outputs.shape, device=device),\n                          create_graph=True, retain_graph=True, only_inputs=True)[0]\n\n    return (pl_grads ** 2).sum(dim=2).mean(dim=1).sqrt()\n\ndef noise(n, latent_dim, device):\n    return torch.randn(n, latent_dim).cuda(device)\n\ndef noise_list(n, layers, latent_dim, device):\n    return [(noise(n, latent_dim, device), layers)]\n\ndef mixed_list(n, layers, latent_dim, device):\n    tt = int(torch.rand(()).numpy() * layers)\n    return noise_list(n, tt, latent_dim, device) + noise_list(n, layers - tt, latent_dim, device)\n\ndef latent_to_w(style_vectorizer, latent_descr):\n    return [(style_vectorizer(z), num_layers) for z, num_layers in latent_descr]\n\ndef image_noise(n, im_size, device):\n    return torch.FloatTensor(n, im_size, im_size, 1).uniform_(0., 1.).cuda(device)\n\ndef leaky_relu(p=0.2):\n    return nn.LeakyReLU(p, inplace=True)\n\ndef evaluate_in_chunks(max_batch_size, model, *args):\n    split_args = list(zip(*list(map(lambda x: x.split(max_batch_size, dim=0), args))))\n    chunked_outputs = [model(*i) for i in split_args]\n    if len(chunked_outputs) == 1:\n        return chunked_outputs[0]\n    return torch.cat(chunked_outputs, dim=0)\n\ndef styles_def_to_tensor(styles_def):\n    return torch.cat([t[:, None, :].expand(-1, n, -1) for t, n in styles_def], dim=1)\n\ndef set_requires_grad(model, bool):\n    for p in model.parameters():\n        p.requires_grad = bool\n\ndef slerp(val, low, high):\n    low_norm = low / torch.norm(low, dim=1, keepdim=True)\n    high_norm = high / torch.norm(high, dim=1, keepdim=True)\n    omega = torch.acos((low_norm * high_norm).sum(1))\n    so = torch.sin(omega)\n    res = (torch.sin((1.0 - val) * omega) / so).unsqueeze(1) * low + (torch.sin(val * omega) / so).unsqueeze(1) * high\n    return res\n\n# losses\n\ndef gen_hinge_loss(fake, real):\n    return fake.mean()\n\ndef hinge_loss(real, fake):\n    return (F.relu(1 + real) + F.relu(1 - fake)).mean()\n\ndef dual_contrastive_loss(real_logits, fake_logits):\n    device = real_logits.device\n    real_logits, fake_logits = map(lambda t: rearrange(t, '... -> (...)'), (real_logits, fake_logits))\n\n    def loss_half(t1, t2):\n        t1 = rearrange(t1, 'i -> i ()')\n        t2 = repeat(t2, 'j -> i j', i = t1.shape[0])\n        t = torch.cat((t1, t2), dim = -1)\n        return F.cross_entropy(t, torch.zeros(t1.shape[0], device = device, dtype = torch.long))\n\n    return loss_half(real_logits, fake_logits) + loss_half(-fake_logits, -real_logits)\n\n# dataset\n\ndef convert_rgb_to_transparent(image):\n    if image.mode != 'RGBA':\n        return image.convert('RGBA')\n    return image\n\ndef convert_transparent_to_rgb(image):\n    if image.mode != 'RGB':\n        return image.convert('RGB')\n    return image\n\nclass expand_greyscale(object):\n    def __init__(self, transparent):\n        self.transparent = transparent\n\n    def __call__(self, tensor):\n        channels = tensor.shape[0]\n        num_target_channels = 4 if self.transparent else 3\n\n        if channels == num_target_channels:\n            return tensor\n\n        alpha = None\n        if channels == 1:\n            color = tensor.expand(3, -1, -1)\n        elif channels == 2:\n            color = tensor[:1].expand(3, -1, -1)\n            alpha = tensor[1:]\n        else:\n            raise Exception(f'image with invalid number of channels given {channels}')\n\n        if not exists(alpha) and self.transparent:\n            alpha = torch.ones(1, *tensor.shape[1:], device=tensor.device)\n\n        return color if not self.transparent else torch.cat((color, alpha))\n\ndef resize_to_minimum_size(min_size, image):\n    if max(*image.size) < min_size:\n        return torchvision.transforms.functional.resize(image, min_size)\n    return image\n\nclass Dataset(data.Dataset):\n    def __init__(self, folder, image_size, transparent = False, aug_prob = 0.):\n        super().__init__()\n        self.folder = folder\n        self.image_size = image_size\n        self.paths = [p for ext in EXTS for p in Path(f'{folder}').glob(f'**/*.{ext}')]\n        assert len(self.paths) > 0, f'No images were found in {folder} for training'\n\n        convert_image_fn = convert_transparent_to_rgb if not transparent else convert_rgb_to_transparent\n        num_channels = 3 if not transparent else 4\n\n        self.transform = transforms.Compose([\n            transforms.Lambda(convert_image_fn),\n            transforms.Lambda(partial(resize_to_minimum_size, image_size)),\n            transforms.Resize(image_size),\n            RandomApply(aug_prob, transforms.RandomResizedCrop(image_size, scale=(0.5, 1.0), ratio=(0.98, 1.02)), transforms.CenterCrop(image_size)),\n            transforms.ToTensor(),\n            transforms.Lambda(expand_greyscale(transparent))\n        ])\n\n    def __len__(self):\n        return len(self.paths)\n\n    def __getitem__(self, index):\n        path = self.paths[index]\n        img = Image.open(path)\n        return self.transform(img)\n\n# augmentations\n\ndef random_hflip(tensor, prob):\n    if prob < random():\n        return tensor\n    return torch.flip(tensor, dims=(3,))\n\nclass AugWrapper(nn.Module):\n    def __init__(self, D, image_size):\n        super().__init__()\n        self.D = D\n\n    def forward(self, images, prob = 0., types = [], detach = False):\n        if random() < prob:\n            images = random_hflip(images, prob=0.5)\n            images = DiffAugment(images, types=types)\n\n        if detach:\n            images = images.detach()\n\n        return self.D(images)\n\n# stylegan2 classes\n\nclass EqualLinear(nn.Module):\n    def __init__(self, in_dim, out_dim, lr_mul = 1, bias = True):\n        super().__init__()\n        self.weight = nn.Parameter(torch.randn(out_dim, in_dim))\n        if bias:\n            self.bias = nn.Parameter(torch.zeros(out_dim))\n\n        self.lr_mul = lr_mul\n\n    def forward(self, input):\n        return F.linear(input, self.weight * self.lr_mul, bias=self.bias * self.lr_mul)\n\nclass StyleVectorizer(nn.Module):\n    def __init__(self, emb, depth, lr_mul = 0.1):\n        super().__init__()\n\n        layers = []\n        for i in range(depth):\n            layers.extend([EqualLinear(emb, emb, lr_mul), leaky_relu()])\n\n        self.net = nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = F.normalize(x, dim=1)\n        return self.net(x)\n\nclass RGBBlock(nn.Module):\n    def __init__(self, latent_dim, input_channel, upsample, rgba = False):\n        super().__init__()\n        self.input_channel = input_channel\n        self.to_style = nn.Linear(latent_dim, input_channel)\n\n        out_filters = 3 if not rgba else 4\n        self.conv = Conv2DMod(input_channel, out_filters, 1, demod=False)\n\n        self.upsample = nn.Sequential(\n            nn.Upsample(scale_factor = 2, mode='bilinear', align_corners=False),\n            Blur()\n        ) if upsample else None\n\n    def forward(self, x, prev_rgb, istyle):\n        b, c, h, w = x.shape\n        style = self.to_style(istyle)\n        x = self.conv(x, style)\n\n        if exists(prev_rgb):\n            x = x + prev_rgb\n\n        if exists(self.upsample):\n            x = self.upsample(x)\n\n        return x\n\nclass Conv2DMod(nn.Module):\n    def __init__(self, in_chan, out_chan, kernel, demod=True, stride=1, dilation=1, eps = 1e-8, **kwargs):\n        super().__init__()\n        self.filters = out_chan\n        self.demod = demod\n        self.kernel = kernel\n        self.stride = stride\n        self.dilation = dilation\n        self.weight = nn.Parameter(torch.randn((out_chan, in_chan, kernel, kernel)))\n        self.eps = eps\n        nn.init.kaiming_normal_(self.weight, a=0, mode='fan_in', nonlinearity='leaky_relu')\n\n    def _get_same_padding(self, size, kernel, dilation, stride):\n        return ((size - 1) * (stride - 1) + dilation * (kernel - 1)) // 2\n\n    def forward(self, x, y):\n        b, c, h, w = x.shape\n\n        w1 = y[:, None, :, None, None]\n        w2 = self.weight[None, :, :, :, :]\n        weights = w2 * (w1 + 1)\n\n        if self.demod:\n            d = torch.rsqrt((weights ** 2).sum(dim=(2, 3, 4), keepdim=True) + self.eps)\n            weights = weights * d\n\n        x = x.reshape(1, -1, h, w)\n\n        _, _, *ws = weights.shape\n        weights = weights.reshape(b * self.filters, *ws)\n\n        padding = self._get_same_padding(h, self.kernel, self.dilation, self.stride)\n        x = F.conv2d(x, weights, padding=padding, groups=b)\n\n        x = x.reshape(-1, self.filters, h, w)\n        return x\n\nclass GeneratorBlock(nn.Module):\n    def __init__(self, latent_dim, input_channels, filters, upsample = True, upsample_rgb = True, rgba = False):\n        super().__init__()\n        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False) if upsample else None\n\n        self.to_style1 = nn.Linear(latent_dim, input_channels)\n        self.to_noise1 = nn.Linear(1, filters)\n        self.conv1 = Conv2DMod(input_channels, filters, 3)\n        \n        self.to_style2 = nn.Linear(latent_dim, filters)\n        self.to_noise2 = nn.Linear(1, filters)\n        self.conv2 = Conv2DMod(filters, filters, 3)\n\n        self.activation = leaky_relu()\n        self.to_rgb = RGBBlock(latent_dim, filters, upsample_rgb, rgba)\n\n    def forward(self, x, prev_rgb, istyle, inoise):\n        if exists(self.upsample):\n            x = self.upsample(x)\n\n        inoise = inoise[:, :x.shape[2], :x.shape[3], :]\n        noise1 = self.to_noise1(inoise).permute((0, 3, 2, 1))\n        noise2 = self.to_noise2(inoise).permute((0, 3, 2, 1))\n\n        style1 = self.to_style1(istyle)\n        x = self.conv1(x, style1)\n        x = self.activation(x + noise1)\n\n        style2 = self.to_style2(istyle)\n        x = self.conv2(x, style2)\n        x = self.activation(x + noise2)\n\n        rgb = self.to_rgb(x, prev_rgb, istyle)\n        return x, rgb\n\nclass DiscriminatorBlock(nn.Module):\n    def __init__(self, input_channels, filters, downsample=True):\n        super().__init__()\n        self.conv_res = nn.Conv2d(input_channels, filters, 1, stride = (2 if downsample else 1))\n\n        self.net = nn.Sequential(\n            nn.Conv2d(input_channels, filters, 3, padding=1),\n            leaky_relu(),\n            nn.Conv2d(filters, filters, 3, padding=1),\n            leaky_relu()\n        )\n\n        self.downsample = nn.Sequential(\n            Blur(),\n            nn.Conv2d(filters, filters, 3, padding = 1, stride = 2)\n        ) if downsample else None\n\n    def forward(self, x):\n        res = self.conv_res(x)\n        x = self.net(x)\n        if exists(self.downsample):\n            x = self.downsample(x)\n        x = (x + res) * (1 / math.sqrt(2))\n        return x\n\nclass Generator(nn.Module):\n    def __init__(self, image_size, latent_dim, network_capacity = 16, transparent = False, attn_layers = [], no_const = False, fmap_max = 512):\n        super().__init__()\n        self.image_size = image_size\n        self.latent_dim = latent_dim\n        self.num_layers = int(log2(image_size) - 1)\n\n        filters = [network_capacity * (2 ** (i + 1)) for i in range(self.num_layers)][::-1]\n\n        set_fmap_max = partial(min, fmap_max)\n        filters = list(map(set_fmap_max, filters))\n        init_channels = filters[0]\n        filters = [init_channels, *filters]\n\n        in_out_pairs = zip(filters[:-1], filters[1:])\n        self.no_const = no_const\n\n        if no_const:\n            self.to_initial_block = nn.ConvTranspose2d(latent_dim, init_channels, 4, 1, 0, bias=False)\n        else:\n            self.initial_block = nn.Parameter(torch.randn((1, init_channels, 4, 4)))\n\n        self.initial_conv = nn.Conv2d(filters[0], filters[0], 3, padding=1)\n        self.blocks = nn.ModuleList([])\n        self.attns = nn.ModuleList([])\n\n        for ind, (in_chan, out_chan) in enumerate(in_out_pairs):\n            not_first = ind != 0\n            not_last = ind != (self.num_layers - 1)\n            num_layer = self.num_layers - ind\n\n            attn_fn = attn_and_ff(in_chan) if num_layer in attn_layers else None\n\n            self.attns.append(attn_fn)\n\n            block = GeneratorBlock(\n                latent_dim,\n                in_chan,\n                out_chan,\n                upsample = not_first,\n                upsample_rgb = not_last,\n                rgba = transparent\n            )\n            self.blocks.append(block)\n\n    def forward(self, styles, input_noise):\n        batch_size = styles.shape[0]\n        image_size = self.image_size\n\n        if self.no_const:\n            avg_style = styles.mean(dim=1)[:, :, None, None]\n            x = self.to_initial_block(avg_style)\n        else:\n            x = self.initial_block.expand(batch_size, -1, -1, -1)\n\n        rgb = None\n        styles = styles.transpose(0, 1)\n        x = self.initial_conv(x)\n\n        for style, block, attn in zip(styles, self.blocks, self.attns):\n            if exists(attn):\n                x = attn(x)\n            x, rgb = block(x, rgb, style, input_noise)\n\n        return rgb\n\nclass Discriminator(nn.Module):\n    def __init__(self, image_size, network_capacity = 16, fq_layers = [], fq_dict_size = 256, attn_layers = [], transparent = False, fmap_max = 512):\n        super().__init__()\n        num_layers = int(log2(image_size) - 1)\n        num_init_filters = 3 if not transparent else 4\n\n        blocks = []\n        filters = [num_init_filters] + [(network_capacity * 4) * (2 ** i) for i in range(num_layers + 1)]\n\n        set_fmap_max = partial(min, fmap_max)\n        filters = list(map(set_fmap_max, filters))\n        chan_in_out = list(zip(filters[:-1], filters[1:]))\n\n        blocks = []\n        attn_blocks = []\n        quantize_blocks = []\n\n        for ind, (in_chan, out_chan) in enumerate(chan_in_out):\n            num_layer = ind + 1\n            is_not_last = ind != (len(chan_in_out) - 1)\n\n            block = DiscriminatorBlock(in_chan, out_chan, downsample = is_not_last)\n            blocks.append(block)\n\n            attn_fn = attn_and_ff(out_chan) if num_layer in attn_layers else None\n\n            attn_blocks.append(attn_fn)\n\n            quantize_fn = PermuteToFrom(VectorQuantize(out_chan, fq_dict_size)) if num_layer in fq_layers else None\n            quantize_blocks.append(quantize_fn)\n\n        self.blocks = nn.ModuleList(blocks)\n        self.attn_blocks = nn.ModuleList(attn_blocks)\n        self.quantize_blocks = nn.ModuleList(quantize_blocks)\n\n        chan_last = filters[-1]\n        latent_dim = 2 * 2 * chan_last\n\n        self.final_conv = nn.Conv2d(chan_last, chan_last, 3, padding=1)\n        self.flatten = Flatten()\n        self.to_logit = nn.Linear(latent_dim, 1)\n\n    def forward(self, x):\n        b, *_ = x.shape\n\n        quantize_loss = torch.zeros(1).to(x)\n\n        for (block, attn_block, q_block) in zip(self.blocks, self.attn_blocks, self.quantize_blocks):\n            x = block(x)\n\n            if exists(attn_block):\n                x = attn_block(x)\n\n            if exists(q_block):\n                x, loss = q_block(x)\n                quantize_loss += loss\n\n        x = self.final_conv(x)\n        x = self.flatten(x)\n        x = self.to_logit(x)\n        return x.squeeze(), quantize_loss\n\nclass StyleGAN2(nn.Module):\n    def __init__(self, image_size, latent_dim = 512, fmap_max = 512, style_depth = 8, network_capacity = 16, transparent = False, fp16 = False, cl_reg = False, steps = 1, lr = 1e-4, ttur_mult = 2, fq_layers = [], fq_dict_size = 256, attn_layers = [], no_const = False, lr_mlp = 0.1, rank = 0):\n        super().__init__()\n        self.lr = lr\n        self.steps = steps\n        self.ema_updater = EMA(0.995)\n\n        self.S = StyleVectorizer(latent_dim, style_depth, lr_mul = lr_mlp)\n        self.G = Generator(image_size, latent_dim, network_capacity, transparent = transparent, attn_layers = attn_layers, no_const = no_const, fmap_max = fmap_max)\n        self.D = Discriminator(image_size, network_capacity, fq_layers = fq_layers, fq_dict_size = fq_dict_size, attn_layers = attn_layers, transparent = transparent, fmap_max = fmap_max)\n\n        self.SE = StyleVectorizer(latent_dim, style_depth, lr_mul = lr_mlp)\n        self.GE = Generator(image_size, latent_dim, network_capacity, transparent = transparent, attn_layers = attn_layers, no_const = no_const)\n\n        self.D_cl = None\n\n        if cl_reg:\n            from contrastive_learner import ContrastiveLearner\n            # experimental contrastive loss discriminator regularization\n            assert not transparent, 'contrastive loss regularization does not work with transparent images yet'\n            self.D_cl = ContrastiveLearner(self.D, image_size, hidden_layer='flatten')\n\n        # wrapper for augmenting all images going into the discriminator\n        self.D_aug = AugWrapper(self.D, image_size)\n\n        # turn off grad for exponential moving averages\n        set_requires_grad(self.SE, False)\n        set_requires_grad(self.GE, False)\n\n        # init optimizers\n        generator_params = list(self.G.parameters()) + list(self.S.parameters())\n        self.G_opt = Adam(generator_params, lr = self.lr, betas=(0.5, 0.9))\n        self.D_opt = Adam(self.D.parameters(), lr = self.lr * ttur_mult, betas=(0.5, 0.9))\n\n        # init weights\n        self._init_weights()\n        self.reset_parameter_averaging()\n\n        self.cuda(rank)\n\n        # startup apex mixed precision\n        self.fp16 = fp16\n        if fp16:\n            (self.S, self.G, self.D, self.SE, self.GE), (self.G_opt, self.D_opt) = amp.initialize([self.S, self.G, self.D, self.SE, self.GE], [self.G_opt, self.D_opt], opt_level='O1', num_losses=3)\n\n    def _init_weights(self):\n        for m in self.modules():\n            if type(m) in {nn.Conv2d, nn.Linear}:\n                nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in', nonlinearity='leaky_relu')\n\n        for block in self.G.blocks:\n            nn.init.zeros_(block.to_noise1.weight)\n            nn.init.zeros_(block.to_noise2.weight)\n            nn.init.zeros_(block.to_noise1.bias)\n            nn.init.zeros_(block.to_noise2.bias)\n\n    def EMA(self):\n        def update_moving_average(ma_model, current_model):\n            for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n                old_weight, up_weight = ma_params.data, current_params.data\n                ma_params.data = self.ema_updater.update_average(old_weight, up_weight)\n\n        update_moving_average(self.SE, self.S)\n        update_moving_average(self.GE, self.G)\n\n    def reset_parameter_averaging(self):\n        self.SE.load_state_dict(self.S.state_dict())\n        self.GE.load_state_dict(self.G.state_dict())\n\n    def forward(self, x):\n        return x\n\nclass Trainer():\n    def __init__(\n        self,\n        name = 'default',\n        results_dir = 'results',\n        models_dir = 'models',\n        base_dir = './',\n        image_size = 128,\n        network_capacity = 16,\n        fmap_max = 512,\n        transparent = False,\n        batch_size = 4,\n        mixed_prob = 0.9,\n        gradient_accumulate_every=1,\n        lr = 2e-4,\n        lr_mlp = 0.1,\n        ttur_mult = 2,\n        rel_disc_loss = False,\n        num_workers = None,\n        save_every = 1000,\n        evaluate_every = 1000,\n        num_image_tiles = 8,\n        trunc_psi = 0.6,\n        fp16 = False,\n        cl_reg = False,\n        no_pl_reg = False,\n        fq_layers = [],\n        fq_dict_size = 256,\n        attn_layers = [],\n        no_const = False,\n        aug_prob = 0.,\n        aug_types = ['translation', 'cutout'],\n        top_k_training = False,\n        generator_top_k_gamma = 0.99,\n        generator_top_k_frac = 0.5,\n        dual_contrast_loss = False,\n        dataset_aug_prob = 0.,\n        calculate_fid_every = None,\n        calculate_fid_num_images = 12800,\n        clear_fid_cache = False,\n        is_ddp = False,\n        rank = 0,\n        world_size = 1,\n        log = False,\n        *args,\n        **kwargs\n    ):\n        self.GAN_params = [args, kwargs]\n        self.GAN = None\n\n        self.name = name\n\n        base_dir = Path(base_dir)\n        self.base_dir = base_dir\n        self.results_dir = base_dir / results_dir\n        self.models_dir = base_dir / models_dir\n        self.fid_dir = base_dir / 'fid' / name\n        self.config_path = self.models_dir / name / '.config.json'\n\n        assert log2(image_size).is_integer(), 'image size must be a power of 2 (64, 128, 256, 512, 1024)'\n        self.image_size = image_size\n        self.network_capacity = network_capacity\n        self.fmap_max = fmap_max\n        self.transparent = transparent\n\n        self.fq_layers = cast_list(fq_layers)\n        self.fq_dict_size = fq_dict_size\n        self.has_fq = len(self.fq_layers) > 0\n\n        self.attn_layers = cast_list(attn_layers)\n        self.no_const = no_const\n\n        self.aug_prob = aug_prob\n        self.aug_types = aug_types\n\n        self.lr = lr\n        self.lr_mlp = lr_mlp\n        self.ttur_mult = ttur_mult\n        self.rel_disc_loss = rel_disc_loss\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.mixed_prob = mixed_prob\n\n        self.num_image_tiles = num_image_tiles\n        self.evaluate_every = evaluate_every\n        self.save_every = save_every\n        self.steps = 0\n\n        self.av = None\n        self.trunc_psi = trunc_psi\n\n        self.no_pl_reg = no_pl_reg\n        self.pl_mean = None\n\n        self.gradient_accumulate_every = gradient_accumulate_every\n\n        assert not fp16 or fp16 and APEX_AVAILABLE, 'Apex is not available for you to use mixed precision training'\n        self.fp16 = fp16\n\n        self.cl_reg = cl_reg\n\n        self.d_loss = 0\n        self.g_loss = 0\n        self.q_loss = None\n        self.last_gp_loss = None\n        self.last_cr_loss = None\n        self.last_fid = None\n\n        self.pl_length_ma = EMA(0.99)\n        self.init_folders()\n\n        self.loader = None\n        self.dataset_aug_prob = dataset_aug_prob\n\n        self.calculate_fid_every = calculate_fid_every\n        self.calculate_fid_num_images = calculate_fid_num_images\n        self.clear_fid_cache = clear_fid_cache\n\n        self.top_k_training = top_k_training\n        self.generator_top_k_gamma = generator_top_k_gamma\n        self.generator_top_k_frac = generator_top_k_frac\n\n        self.dual_contrast_loss = dual_contrast_loss\n\n        assert not (is_ddp and cl_reg), 'Contrastive loss regularization does not work well with multi GPUs yet'\n        self.is_ddp = is_ddp\n        self.is_main = rank == 0\n        self.rank = rank\n        self.world_size = world_size\n\n        self.logger = aim.Session(experiment=name) if log else None\n\n    @property\n    def image_extension(self):\n        return 'jpg' if not self.transparent else 'png'\n\n    @property\n    def checkpoint_num(self):\n        return floor(self.steps // self.save_every)\n\n    @property\n    def hparams(self):\n        return {'image_size': self.image_size, 'network_capacity': self.network_capacity}\n        \n    def init_GAN(self):\n        args, kwargs = self.GAN_params\n        self.GAN = StyleGAN2(lr = self.lr, lr_mlp = self.lr_mlp, ttur_mult = self.ttur_mult, image_size = self.image_size, network_capacity = self.network_capacity, fmap_max = self.fmap_max, transparent = self.transparent, fq_layers = self.fq_layers, fq_dict_size = self.fq_dict_size, attn_layers = self.attn_layers, fp16 = self.fp16, cl_reg = self.cl_reg, no_const = self.no_const, rank = self.rank, *args, **kwargs)\n\n        if self.is_ddp:\n            ddp_kwargs = {'device_ids': [self.rank]}\n            self.S_ddp = DDP(self.GAN.S, **ddp_kwargs)\n            self.G_ddp = DDP(self.GAN.G, **ddp_kwargs)\n            self.D_ddp = DDP(self.GAN.D, **ddp_kwargs)\n            self.D_aug_ddp = DDP(self.GAN.D_aug, **ddp_kwargs)\n\n        if exists(self.logger):\n            self.logger.set_params(self.hparams)\n\n    def write_config(self):\n        self.config_path.write_text(json.dumps(self.config()))\n\n    def load_config(self):\n        config = self.config() if not self.config_path.exists() else json.loads(self.config_path.read_text())\n        self.image_size = config['image_size']\n        self.network_capacity = config['network_capacity']\n        self.transparent = config['transparent']\n        self.fq_layers = config['fq_layers']\n        self.fq_dict_size = config['fq_dict_size']\n        self.fmap_max = config.pop('fmap_max', 512)\n        self.attn_layers = config.pop('attn_layers', [])\n        self.no_const = config.pop('no_const', False)\n        self.lr_mlp = config.pop('lr_mlp', 0.1)\n        del self.GAN\n        self.init_GAN()\n\n    def config(self):\n        return {'image_size': self.image_size, 'network_capacity': self.network_capacity, 'lr_mlp': self.lr_mlp, 'transparent': self.transparent, 'fq_layers': self.fq_layers, 'fq_dict_size': self.fq_dict_size, 'attn_layers': self.attn_layers, 'no_const': self.no_const}\n\n    def set_data_src(self, folder):\n        self.dataset = Dataset(folder, self.image_size, transparent = self.transparent, aug_prob = self.dataset_aug_prob)\n        num_workers = num_workers = default(self.num_workers, NUM_CORES if not self.is_ddp else 0)\n        sampler = DistributedSampler(self.dataset, rank=self.rank, num_replicas=self.world_size, shuffle=True) if self.is_ddp else None\n        dataloader = data.DataLoader(self.dataset, num_workers = num_workers, batch_size = math.ceil(self.batch_size / self.world_size), sampler = sampler, shuffle = not self.is_ddp, drop_last = True, pin_memory = True)\n        self.loader = cycle(dataloader)\n\n        # auto set augmentation prob for user if dataset is detected to be low\n        num_samples = len(self.dataset)\n        if not exists(self.aug_prob) and num_samples < 1e5:\n            self.aug_prob = min(0.5, (1e5 - num_samples) * 3e-6)\n            print(f'autosetting augmentation probability to {round(self.aug_prob * 100)}%')\n\n    def train(self):\n        assert exists(self.loader), 'You must first initialize the data source with `.set_data_src(<folder of images>)`'\n\n        if not exists(self.GAN):\n            self.init_GAN()\n\n        self.GAN.train()\n        total_disc_loss = torch.tensor(0.).cuda(self.rank)\n        total_gen_loss = torch.tensor(0.).cuda(self.rank)\n\n        batch_size = math.ceil(self.batch_size / self.world_size)\n\n        image_size = self.GAN.G.image_size\n        latent_dim = self.GAN.G.latent_dim\n        num_layers = self.GAN.G.num_layers\n\n        aug_prob   = self.aug_prob\n        aug_types  = self.aug_types\n        aug_kwargs = {'prob': aug_prob, 'types': aug_types}\n\n        apply_gradient_penalty = self.steps % 4 == 0\n        apply_path_penalty = not self.no_pl_reg and self.steps > 5000 and self.steps % 32 == 0\n        apply_cl_reg_to_generated = self.steps > 20000\n\n        S = self.GAN.S if not self.is_ddp else self.S_ddp\n        G = self.GAN.G if not self.is_ddp else self.G_ddp\n        D = self.GAN.D if not self.is_ddp else self.D_ddp\n        D_aug = self.GAN.D_aug if not self.is_ddp else self.D_aug_ddp\n\n        backwards = partial(loss_backwards, self.fp16)\n\n        if exists(self.GAN.D_cl):\n            self.GAN.D_opt.zero_grad()\n\n            if apply_cl_reg_to_generated:\n                for i in range(self.gradient_accumulate_every):\n                    get_latents_fn = mixed_list if random() < self.mixed_prob else noise_list\n                    style = get_latents_fn(batch_size, num_layers, latent_dim, device=self.rank)\n                    noise = image_noise(batch_size, image_size, device=self.rank)\n\n                    w_space = latent_to_w(self.GAN.S, style)\n                    w_styles = styles_def_to_tensor(w_space)\n\n                    generated_images = self.GAN.G(w_styles, noise)\n                    self.GAN.D_cl(generated_images.clone().detach(), accumulate=True)\n\n            for i in range(self.gradient_accumulate_every):\n                image_batch = next(self.loader).cuda(self.rank)\n                self.GAN.D_cl(image_batch, accumulate=True)\n\n            loss = self.GAN.D_cl.calculate_loss()\n            self.last_cr_loss = loss.clone().detach().item()\n            backwards(loss, self.GAN.D_opt, loss_id = 0)\n\n            self.GAN.D_opt.step()\n\n        # setup losses\n\n        if not self.dual_contrast_loss:\n            D_loss_fn = hinge_loss\n            G_loss_fn = gen_hinge_loss\n            G_requires_reals = False\n        else:\n            D_loss_fn = dual_contrastive_loss\n            G_loss_fn = dual_contrastive_loss\n            G_requires_reals = True\n\n        # train discriminator\n\n        avg_pl_length = self.pl_mean\n        self.GAN.D_opt.zero_grad()\n\n        for i in gradient_accumulate_contexts(self.gradient_accumulate_every, self.is_ddp, ddps=[D_aug, S, G]):\n            get_latents_fn = mixed_list if random() < self.mixed_prob else noise_list\n            style = get_latents_fn(batch_size, num_layers, latent_dim, device=self.rank)\n            noise = image_noise(batch_size, image_size, device=self.rank)\n\n            w_space = latent_to_w(S, style)\n            w_styles = styles_def_to_tensor(w_space)\n\n            generated_images = G(w_styles, noise)\n            fake_output, fake_q_loss = D_aug(generated_images.clone().detach(), detach = True, **aug_kwargs)\n\n            image_batch = next(self.loader).cuda(self.rank)\n            image_batch.requires_grad_()\n            real_output, real_q_loss = D_aug(image_batch, **aug_kwargs)\n\n            real_output_loss = real_output\n            fake_output_loss = fake_output\n\n            if self.rel_disc_loss:\n                real_output_loss = real_output_loss - fake_output.mean()\n                fake_output_loss = fake_output_loss - real_output.mean()\n\n            divergence = D_loss_fn(real_output_loss, fake_output_loss)\n            disc_loss = divergence\n\n            if self.has_fq:\n                quantize_loss = (fake_q_loss + real_q_loss).mean()\n                self.q_loss = float(quantize_loss.detach().item())\n\n                disc_loss = disc_loss + quantize_loss\n\n            if apply_gradient_penalty:\n                gp = gradient_penalty(image_batch, real_output)\n                self.last_gp_loss = gp.clone().detach().item()\n                self.track(self.last_gp_loss, 'GP')\n                disc_loss = disc_loss + gp\n\n            disc_loss = disc_loss / self.gradient_accumulate_every\n            disc_loss.register_hook(raise_if_nan)\n            backwards(disc_loss, self.GAN.D_opt, loss_id = 1)\n\n            total_disc_loss += divergence.detach().item() / self.gradient_accumulate_every\n\n        self.d_loss = float(total_disc_loss)\n        self.track(self.d_loss, 'D')\n\n        self.GAN.D_opt.step()\n\n        # train generator\n\n        self.GAN.G_opt.zero_grad()\n\n        for i in gradient_accumulate_contexts(self.gradient_accumulate_every, self.is_ddp, ddps=[S, G, D_aug]):\n            style = get_latents_fn(batch_size, num_layers, latent_dim, device=self.rank)\n            noise = image_noise(batch_size, image_size, device=self.rank)\n\n            w_space = latent_to_w(S, style)\n            w_styles = styles_def_to_tensor(w_space)\n\n            generated_images = G(w_styles, noise)\n            fake_output, _ = D_aug(generated_images, **aug_kwargs)\n            fake_output_loss = fake_output\n\n            real_output = None\n            if G_requires_reals:\n                image_batch = next(self.loader).cuda(self.rank)\n                real_output, _ = D_aug(image_batch, detach = True, **aug_kwargs)\n                real_output = real_output.detach()\n\n            if self.top_k_training:\n                epochs = (self.steps * batch_size * self.gradient_accumulate_every) / len(self.dataset)\n                k_frac = max(self.generator_top_k_gamma ** epochs, self.generator_top_k_frac)\n                k = math.ceil(batch_size * k_frac)\n\n                if k != batch_size:\n                    fake_output_loss, _ = fake_output_loss.topk(k=k, largest=False)\n\n            loss = G_loss_fn(fake_output_loss, real_output)\n            gen_loss = loss\n\n            if apply_path_penalty:\n                pl_lengths = calc_pl_lengths(w_styles, generated_images)\n                avg_pl_length = np.mean(pl_lengths.detach().cpu().numpy())\n\n                if not is_empty(self.pl_mean):\n                    pl_loss = ((pl_lengths - self.pl_mean) ** 2).mean()\n                    if not torch.isnan(pl_loss):\n                        gen_loss = gen_loss + pl_loss\n\n            gen_loss = gen_loss / self.gradient_accumulate_every\n            gen_loss.register_hook(raise_if_nan)\n            backwards(gen_loss, self.GAN.G_opt, loss_id = 2)\n\n            total_gen_loss += loss.detach().item() / self.gradient_accumulate_every\n\n        self.g_loss = float(total_gen_loss)\n        self.track(self.g_loss, 'G')\n\n        self.GAN.G_opt.step()\n\n        # calculate moving averages\n\n        if apply_path_penalty and not np.isnan(avg_pl_length):\n            self.pl_mean = self.pl_length_ma.update_average(self.pl_mean, avg_pl_length)\n            self.track(self.pl_mean, 'PL')\n\n        if self.is_main and self.steps % 10 == 0 and self.steps > 20000:\n            self.GAN.EMA()\n\n        if self.is_main and self.steps <= 25000 and self.steps % 1000 == 2:\n            self.GAN.reset_parameter_averaging()\n\n        # save from NaN errors\n\n        if any(torch.isnan(l) for l in (total_gen_loss, total_disc_loss)):\n            print(f'NaN detected for generator or discriminator. Loading from checkpoint #{self.checkpoint_num}')\n            self.load(self.checkpoint_num)\n            raise NanException\n\n        # periodically save results\n\n        if self.is_main:\n            if self.steps % self.save_every == 0:\n                self.save(self.checkpoint_num)\n\n            if self.steps % self.evaluate_every == 0 or (self.steps % 100 == 0 and self.steps < 2500):\n                self.evaluate(floor(self.steps / self.evaluate_every))\n\n            if exists(self.calculate_fid_every) and self.steps % self.calculate_fid_every == 0 and self.steps != 0:\n                num_batches = math.ceil(self.calculate_fid_num_images / self.batch_size)\n                fid = self.calculate_fid(num_batches)\n                self.last_fid = fid\n\n                with open(str(self.results_dir / self.name / f'fid_scores.txt'), 'a') as f:\n                    f.write(f'{self.steps},{fid}\\n')\n\n        self.steps += 1\n        self.av = None\n\n    @torch.no_grad()\n    def evaluate(self, num = 0, trunc = 1.0):\n        self.GAN.eval()\n        ext = self.image_extension\n        num_rows = self.num_image_tiles\n    \n        latent_dim = self.GAN.G.latent_dim\n        image_size = self.GAN.G.image_size\n        num_layers = self.GAN.G.num_layers\n\n        # latents and noise\n\n        latents = noise_list(num_rows ** 2, num_layers, latent_dim, device=self.rank)\n        n = image_noise(num_rows ** 2, image_size, device=self.rank)\n\n        # regular\n\n        generated_images = self.generate_truncated(self.GAN.S, self.GAN.G, latents, n, trunc_psi = self.trunc_psi)\n        torchvision.utils.save_image(generated_images, str(self.results_dir / self.name / f'{str(num)}.{ext}'), nrow=num_rows)\n        \n        # moving averages\n\n        generated_images = self.generate_truncated(self.GAN.SE, self.GAN.GE, latents, n, trunc_psi = self.trunc_psi)\n        torchvision.utils.save_image(generated_images, str(self.results_dir / self.name / f'{str(num)}-ema.{ext}'), nrow=num_rows)\n\n        # mixing regularities\n\n        def tile(a, dim, n_tile):\n            init_dim = a.size(dim)\n            repeat_idx = [1] * a.dim()\n            repeat_idx[dim] = n_tile\n            a = a.repeat(*(repeat_idx))\n            order_index = torch.LongTensor(np.concatenate([init_dim * np.arange(n_tile) + i for i in range(init_dim)])).cuda(self.rank)\n            return torch.index_select(a, dim, order_index)\n\n        nn = noise(num_rows, latent_dim, device=self.rank)\n        tmp1 = tile(nn, 0, num_rows)\n        tmp2 = nn.repeat(num_rows, 1)\n\n        tt = int(num_layers / 2)\n        mixed_latents = [(tmp1, tt), (tmp2, num_layers - tt)]\n\n        generated_images = self.generate_truncated(self.GAN.SE, self.GAN.GE, mixed_latents, n, trunc_psi = self.trunc_psi)\n        torchvision.utils.save_image(generated_images, str(self.results_dir / self.name / f'{str(num)}-mr.{ext}'), nrow=num_rows)\n\n    @torch.no_grad()\n    def calculate_fid(self, num_batches):\n        from pytorch_fid import fid_score\n        torch.cuda.empty_cache()\n\n        real_path = self.fid_dir / 'real'\n        fake_path = self.fid_dir / 'fake'\n\n        # remove any existing files used for fid calculation and recreate directories\n\n        if not real_path.exists() or self.clear_fid_cache:\n            rmtree(real_path, ignore_errors=True)\n            os.makedirs(real_path)\n\n            for batch_num in tqdm(range(num_batches), desc='calculating FID - saving reals'):\n                real_batch = next(self.loader)\n                for k, image in enumerate(real_batch.unbind(0)):\n                    filename = str(k + batch_num * self.batch_size)\n                    torchvision.utils.save_image(image, str(real_path / f'{filename}.png'))\n\n        # generate a bunch of fake images in results / name / fid_fake\n\n        rmtree(fake_path, ignore_errors=True)\n        os.makedirs(fake_path)\n\n        self.GAN.eval()\n        ext = self.image_extension\n\n        latent_dim = self.GAN.G.latent_dim\n        image_size = self.GAN.G.image_size\n        num_layers = self.GAN.G.num_layers\n\n        for batch_num in tqdm(range(num_batches), desc='calculating FID - saving generated'):\n            # latents and noise\n            latents = noise_list(self.batch_size, num_layers, latent_dim, device=self.rank)\n            noise = image_noise(self.batch_size, image_size, device=self.rank)\n\n            # moving averages\n            generated_images = self.generate_truncated(self.GAN.SE, self.GAN.GE, latents, noise, trunc_psi = self.trunc_psi)\n\n            for j, image in enumerate(generated_images.unbind(0)):\n                torchvision.utils.save_image(image, str(fake_path / f'{str(j + batch_num * self.batch_size)}-ema.{ext}'))\n\n        return fid_score.calculate_fid_given_paths([str(real_path), str(fake_path)], 256, noise.device, 2048)\n\n    @torch.no_grad()\n    def truncate_style(self, tensor, trunc_psi = 0.75):\n        S = self.GAN.S\n        batch_size = self.batch_size\n        latent_dim = self.GAN.G.latent_dim\n\n        if not exists(self.av):\n            z = noise(2000, latent_dim, device=self.rank)\n            samples = evaluate_in_chunks(batch_size, S, z).cpu().numpy()\n            self.av = np.mean(samples, axis = 0)\n            self.av = np.expand_dims(self.av, axis = 0)\n\n        av_torch = torch.from_numpy(self.av).cuda(self.rank)\n        tensor = trunc_psi * (tensor - av_torch) + av_torch\n        return tensor\n\n    @torch.no_grad()\n    def truncate_style_defs(self, w, trunc_psi = 0.75):\n        w_space = []\n        for tensor, num_layers in w:\n            tensor = self.truncate_style(tensor, trunc_psi = trunc_psi)            \n            w_space.append((tensor, num_layers))\n        return w_space\n\n    @torch.no_grad()\n    def generate_truncated(self, S, G, style, noi, trunc_psi = 0.75, num_image_tiles = 8):\n        w = map(lambda t: (S(t[0]), t[1]), style)\n        w_truncated = self.truncate_style_defs(w, trunc_psi = trunc_psi)\n        w_styles = styles_def_to_tensor(w_truncated)\n        generated_images = evaluate_in_chunks(self.batch_size, G, w_styles, noi)\n        return generated_images.clamp_(0., 1.)\n\n    @torch.no_grad()\n    def generate_interpolation(self, num = 0, num_image_tiles = 8, trunc = 1.0, num_steps = 100, save_frames = False):\n        self.GAN.eval()\n        ext = self.image_extension\n        num_rows = num_image_tiles\n\n        latent_dim = self.GAN.G.latent_dim\n        image_size = self.GAN.G.image_size\n        num_layers = self.GAN.G.num_layers\n\n        # latents and noise\n\n        latents_low = noise(num_rows ** 2, latent_dim, device=self.rank)\n        latents_high = noise(num_rows ** 2, latent_dim, device=self.rank)\n        n = image_noise(num_rows ** 2, image_size, device=self.rank)\n\n        ratios = torch.linspace(0., 8., num_steps)\n\n        frames = []\n        for ratio in tqdm(ratios):\n            interp_latents = slerp(ratio, latents_low, latents_high)\n            latents = [(interp_latents, num_layers)]\n            generated_images = self.generate_truncated(self.GAN.SE, self.GAN.GE, latents, n, trunc_psi = self.trunc_psi)\n            images_grid = torchvision.utils.make_grid(generated_images, nrow = num_rows)\n            pil_image = transforms.ToPILImage()(images_grid.cpu())\n            \n            if self.transparent:\n                background = Image.new(\"RGBA\", pil_image.size, (255, 255, 255))\n                pil_image = Image.alpha_composite(background, pil_image)\n                \n            frames.append(pil_image)\n\n        frames[0].save(str(self.results_dir / self.name / f'{str(num)}.gif'), save_all=True, append_images=frames[1:], duration=80, loop=0, optimize=True)\n\n        if save_frames:\n            folder_path = (self.results_dir / self.name / f'{str(num)}')\n            folder_path.mkdir(parents=True, exist_ok=True)\n            for ind, frame in enumerate(frames):\n                frame.save(str(folder_path / f'{str(ind)}.{ext}'))\n\n    def print_log(self):\n        data = [\n            ('G', self.g_loss),\n            ('D', self.d_loss),\n            ('GP', self.last_gp_loss),\n            ('PL', self.pl_mean),\n            ('CR', self.last_cr_loss),\n            ('Q', self.q_loss),\n            ('FID', self.last_fid)\n        ]\n\n        data = [d for d in data if exists(d[1])]\n        log = ' | '.join(map(lambda n: f'{n[0]}: {n[1]:.2f}', data))\n        print(log)\n\n    def track(self, value, name):\n        if not exists(self.logger):\n            return\n        self.logger.track(value, name = name)\n\n    def model_name(self, num):\n        return str(self.models_dir / self.name / f'model_{num}.pt')\n\n    def init_folders(self):\n        (self.results_dir / self.name).mkdir(parents=True, exist_ok=True)\n        (self.models_dir / self.name).mkdir(parents=True, exist_ok=True)\n\n    def clear(self):\n        rmtree(str(self.models_dir / self.name), True)\n        rmtree(str(self.results_dir / self.name), True)\n        rmtree(str(self.fid_dir), True)\n        rmtree(str(self.config_path), True)\n        self.init_folders()\n\n    def save(self, num):\n        save_data = {\n            'GAN': self.GAN.state_dict(),\n            'version': __version__\n        }\n\n        if self.GAN.fp16:\n            save_data['amp'] = amp.state_dict()\n\n        torch.save(save_data, self.model_name(num))\n        self.write_config()\n\n    def load(self, num = -1):\n        self.load_config()\n\n        name = num\n        if num == -1:\n            file_paths = [p for p in Path(self.models_dir / self.name).glob('model_*.pt')]\n            saved_nums = sorted(map(lambda x: int(x.stem.split('_')[1]), file_paths))\n            if len(saved_nums) == 0:\n                return\n            name = saved_nums[-1]\n            print(f'continuing from previous epoch - {name}')\n\n        self.steps = name * self.save_every\n\n        load_data = torch.load(self.model_name(name))\n\n        if 'version' in load_data:\n            print(f\"loading from version {load_data['version']}\")\n\n        try:\n            self.GAN.load_state_dict(load_data['GAN'])\n        except Exception as e:\n            print('unable to load save model. please try downgrading the package to the version specified by the saved model')\n            raise e\n        if self.GAN.fp16 and 'amp' in load_data:\n            amp.load_state_dict(load_data['amp'])\n        \n    def load_GAN(self):\n        self.load_config()\n        \n        load_data = torch.load(\"/kaggle/input/style2gan-model/model_5 (forth tarin).pt\")\n        try:\n            self.GAN.load_state_dict(load_data[\"GAN\"])\n        except Exception as e:\n            print(\"Unable to load save model\")\n            raise e\n\nclass ModelLoader:\n    def __init__(self, *, base_dir, name = 'default', load_from = -1):\n        self.model = Trainer(name = name, base_dir = base_dir)\n        self.model.load(load_from)\n\n    def noise_to_styles(self, noise, trunc_psi = None):\n        noise = noise.cuda()\n        w = self.model.GAN.SE(noise)\n        if exists(trunc_psi):\n            w = self.model.truncate_style(w)\n        return w\n\n    def styles_to_images(self, w):\n        batch_size, *_ = w.shape\n        num_layers = self.model.GAN.GE.num_layers\n        image_size = self.model.image_size\n        w_def = [(w, num_layers)]\n\n        w_tensors = styles_def_to_tensor(w_def)\n        noise = image_noise(batch_size, image_size, device = 0)\n\n        images = self.model.GAN.GE(w_tensors, noise)\n        images.clamp_(0., 1.)\n        return images","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Trainer","metadata":{}},{"cell_type":"code","source":"from retry.api import retry_call\nfrom datetime import datetime\nfrom functools import wraps\n\nimport torch\nimport torch.multiprocessing as mp\nimport torch.distributed as dist\n\nimport numpy as np\n\ndef cast_list(el):\n    return el if isinstance(el, list) else [el]\n\ndef timestamped_filename(prefix = 'generated-'):\n    now = datetime.now()\n    timestamp = now.strftime(\"%m-%d-%Y_%H-%M-%S\")\n    return f'{prefix}{timestamp}'\n\ndef set_seed(seed):\n    torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    np.random.seed(seed)\n    random.seed(seed)\n\ndef run_training(rank, world_size, model_args, data, load_from, new, num_train_steps, name, seed):\n    is_main = rank == 0\n    is_ddp = world_size > 1\n\n    if is_ddp:\n        set_seed(seed)\n        os.environ['MASTER_ADDR'] = 'localhost'\n        os.environ['MASTER_PORT'] = '12355'\n        dist.init_process_group('nccl', rank=rank, world_size=world_size)\n\n        print(f\"{rank + 1}/{world_size} process initialized.\")\n\n    model_args.update(\n        is_ddp = is_ddp,\n        rank = rank,\n        world_size = world_size\n    )\n\n    model = Trainer(**model_args)\n    model.load_GAN()\n    #if not new:\n        #model.load(load_from)\n    #else:\n        #model.clear()\n\n    model.set_data_src(data)\n\n    progress_bar = tqdm(initial = model.steps, total = num_train_steps, mininterval=10., desc=f'{name}<{data}>')\n    while model.steps < num_train_steps:\n        retry_call(model.train, tries=3, exceptions=NanException)\n        progress_bar.n = model.steps\n        progress_bar.refresh()\n        if is_main and model.steps % 50 == 0:\n            model.print_log()\n\n    model.save(model.checkpoint_num)\n\n    if is_ddp:\n        dist.destroy_process_group()\n\ndef train_from_folder(\n    data = '/kaggle/input/diffusion/faces/faces',\n    results_dir = './results',\n    models_dir = './models',\n    name = 'train again',\n    new = False,\n    load_from = -1,\n    image_size = 64,\n    network_capacity = 16,\n    fmap_max = 512,\n    transparent = False,\n    batch_size = 5,\n    gradient_accumulate_every = 6,\n    num_train_steps = 5001,\n    learning_rate = 2e-4,\n    lr_mlp = 0.1,\n    ttur_mult = 1.5,\n    rel_disc_loss = False,\n    num_workers =  None,\n    save_every = 1000,\n    evaluate_every = 1000,\n    generate = False,\n    num_generate = 1,\n    generate_interpolation = False,\n    interpolation_num_steps = 100,\n    save_frames = False,\n    num_image_tiles = 8,\n    trunc_psi = 0.75,\n    mixed_prob = 0.9,\n    fp16 = False,\n    no_pl_reg = False,\n    cl_reg = False,\n    fq_layers = [],\n    fq_dict_size = 256,\n    attn_layers = [],\n    no_const = False,\n    aug_prob = 0.,\n    aug_types = ['translation', 'cutout'],\n    top_k_training = False,\n    generator_top_k_gamma = 0.99,\n    generator_top_k_frac = 0.5,\n    dual_contrast_loss = False,\n    dataset_aug_prob = 0.,\n    multi_gpus = False,\n    calculate_fid_every = None,\n    calculate_fid_num_images = 12800,\n    clear_fid_cache = False,\n    seed = 42,\n    log = False\n):\n    model_args = dict(\n        name = name,\n        results_dir = results_dir,\n        models_dir = models_dir,\n        batch_size = batch_size,\n        gradient_accumulate_every = gradient_accumulate_every,\n        image_size = image_size,\n        network_capacity = network_capacity,\n        fmap_max = fmap_max,\n        transparent = transparent,\n        lr = learning_rate,\n        lr_mlp = lr_mlp,\n        ttur_mult = ttur_mult,\n        rel_disc_loss = rel_disc_loss,\n        num_workers = num_workers,\n        save_every = save_every,\n        evaluate_every = evaluate_every,\n        num_image_tiles = num_image_tiles,\n        trunc_psi = trunc_psi,\n        fp16 = fp16,\n        no_pl_reg = no_pl_reg,\n        cl_reg = cl_reg,\n        fq_layers = fq_layers,\n        fq_dict_size = fq_dict_size,\n        attn_layers = attn_layers,\n        no_const = no_const,\n        aug_prob = aug_prob,\n        aug_types = cast_list(aug_types),\n        top_k_training = top_k_training,\n        generator_top_k_gamma = generator_top_k_gamma,\n        generator_top_k_frac = generator_top_k_frac,\n        dual_contrast_loss = dual_contrast_loss,\n        dataset_aug_prob = dataset_aug_prob,\n        calculate_fid_every = calculate_fid_every,\n        calculate_fid_num_images = calculate_fid_num_images,\n        clear_fid_cache = clear_fid_cache,\n        mixed_prob = mixed_prob,\n        log = log\n    )\n\n    if generate:\n        model = Trainer(**model_args)\n        model.load(load_from)\n        samples_name = timestamped_filename()\n        for num in tqdm(range(num_generate)):\n            model.evaluate(f'{samples_name}-{num}', num_image_tiles)\n        print(f'sample images generated at {results_dir}/{name}/{samples_name}')\n        return\n\n    if generate_interpolation:\n        model = Trainer(**model_args)\n        model.load(load_from)\n        samples_name = timestamped_filename()\n        model.generate_interpolation(samples_name, num_image_tiles, num_steps = interpolation_num_steps, save_frames = save_frames)\n        print(f'interpolation generated at {results_dir}/{name}/{samples_name}')\n        return\n\n    world_size = torch.cuda.device_count()\n\n    if world_size == 1 or not multi_gpus:\n        run_training(0, 1, model_args, data, load_from, new, num_train_steps, name, seed)\n        return\n\n    mp.spawn(run_training,\n        args=(world_size, model_args, data, load_from, new, num_train_steps, name, seed),\n        nprocs=world_size,\n        join=True)\n\ntrain_from_folder()","metadata":{},"execution_count":null,"outputs":[]}]}
